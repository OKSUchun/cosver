"""
CSV íŒŒì¼ì˜ shop URLì„ ë°©ë¬¸í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

scraper_data.csvì˜ 2í–‰ë¶€í„° 67í–‰ê¹Œì§€ì˜ shop URLì„ ë°©ë¬¸í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³ ,
search_keyword ë³„ë¡œ ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
"""
import csv
import os
import re
import time
import sys
from pathlib import Path
from urllib.parse import urlparse
import requests
from bs4 import BeautifulSoup
from typing import Optional, List

# ìƒí’ˆ ì´ë¯¸ì§€ ì¶”ì¶œ ëª¨ë“ˆ import
sys.path.insert(0, str(Path(__file__).parent.parent / 'src' / 'scraper'))
try:
    from get_product_images import get_product_images, get_musinsa_product_images, get_oliveyoung_product_images
    PRODUCT_IMAGE_MODULE_AVAILABLE = True
except ImportError:
    PRODUCT_IMAGE_MODULE_AVAILABLE = False
    print("âš ï¸  get_product_images ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ë°©ë²•ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")


def sanitize_filename(filename: str) -> str:
    """íŒŒì¼ëª…ì— ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë¬¸ìë¥¼ ì œê±°í•©ë‹ˆë‹¤."""
    # íŒŒì¼ëª…ì— ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë¬¸ì ì œê±°
    filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
    # ê³µë°±ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€ê²½
    filename = re.sub(r'\s+', '_', filename)
    return filename


def sanitize_directory_name(name: str) -> str:
    """ë””ë ‰í† ë¦¬ëª…ì— ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë¬¸ìë¥¼ ì œê±°í•©ë‹ˆë‹¤."""
    return sanitize_filename(name)


def get_image_urls_from_page(url: str, timeout: int = 10) -> List[str]:
    """
    ì›¹ í˜ì´ì§€ì—ì„œ ì´ë¯¸ì§€ URLì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        url: ì›¹ í˜ì´ì§€ URL
        timeout: ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
    
    Returns:
        ì´ë¯¸ì§€ URL ë¦¬ìŠ¤íŠ¸
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=timeout)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        image_urls = []
        
        # img íƒœê·¸ì—ì„œ ì´ë¯¸ì§€ URL ì¶”ì¶œ
        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src') or img.get('data-lazy-src')
            if src:
                # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                if src.startswith('//'):
                    src = 'https:' + src
                elif src.startswith('/'):
                    parsed_url = urlparse(url)
                    src = f"{parsed_url.scheme}://{parsed_url.netloc}{src}"
                elif not src.startswith('http'):
                    parsed_url = urlparse(url)
                    src = f"{parsed_url.scheme}://{parsed_url.netloc}/{src}"
                
                image_urls.append(src)
        
        # ë°°ê²½ ì´ë¯¸ì§€ë¡œ ì‚¬ìš©ëœ ê²½ìš°ë„ ì°¾ê¸°
        for element in soup.find_all(style=True):
            style = element.get('style', '')
            bg_image_match = re.search(r'url\(["\']?([^"\']+)["\']?\)', style)
            if bg_image_match:
                src = bg_image_match.group(1)
                if src.startswith('//'):
                    src = 'https:' + src
                elif src.startswith('/'):
                    parsed_url = urlparse(url)
                    src = f"{parsed_url.scheme}://{parsed_url.netloc}{src}"
                image_urls.append(src)
        
        return list(set(image_urls))  # ì¤‘ë³µ ì œê±°
    except Exception as e:
        print(f"âš ï¸  í˜ì´ì§€ì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨ {url}: {e}")
        return []


def download_image(image_url: str, output_path: Path, timeout: int = 30) -> bool:
    """
    ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        image_url: ì´ë¯¸ì§€ URL
        output_path: ì €ì¥í•  ê²½ë¡œ
        timeout: ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
    
    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Referer': image_url
        }
        response = requests.get(image_url, headers=headers, timeout=timeout, stream=True)
        response.raise_for_status()
        
        # Content-Type í™•ì¸
        content_type = response.headers.get('Content-Type', '')
        if not content_type.startswith('image/'):
            print(f"âš ï¸  ì´ë¯¸ì§€ê°€ ì•„ë‹Œ íŒŒì¼: {image_url} (Content-Type: {content_type})")
            return False
        
        # íŒŒì¼ í™•ì¥ì ê²°ì •
        ext = '.jpg'
        if 'jpeg' in content_type:
            ext = '.jpg'
        elif 'png' in content_type:
            ext = '.png'
        elif 'gif' in content_type:
            ext = '.gif'
        elif 'webp' in content_type:
            ext = '.webp'
        else:
            # URLì—ì„œ í™•ì¥ì ì¶”ì¶œ
            parsed = urlparse(image_url)
            path_ext = Path(parsed.path).suffix.lower()
            if path_ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']:
                ext = path_ext
        
        # íŒŒì¼ëª… ìƒì„±
        if not output_path.suffix:
            output_path = output_path.with_suffix(ext)
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
        with open(output_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        
        return True
    except Exception as e:
        print(f"âš ï¸  ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ {image_url}: {e}")
        return False


def download_images_from_csv(
    csv_path: str,
    output_base_dir: str = "downloaded_images",
    start_row: int = 2,
    end_row: int = 67,
    delay: float = 1.0
):
    """
    CSV íŒŒì¼ì˜ shop URLì„ ë°©ë¬¸í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        csv_path: CSV íŒŒì¼ ê²½ë¡œ
        output_base_dir: ì´ë¯¸ì§€ë¥¼ ì €ì¥í•  ê¸°ë³¸ ë””ë ‰í† ë¦¬
        start_row: ì‹œì‘ í–‰ (1-based, í—¤ë” í¬í•¨)
        end_row: ë í–‰ (1-based, í—¤ë” í¬í•¨)
        delay: ìš”ì²­ ê°„ ì§€ì—° ì‹œê°„ (ì´ˆ)
    """
    csv_path = Path(csv_path)
    if not csv_path.exists():
        print(f"âŒ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
        return
    
    output_base = Path(output_base_dir)
    output_base.mkdir(parents=True, exist_ok=True)
    
    print(f"ğŸ“‚ CSV íŒŒì¼ ì½ê¸°: {csv_path}")
    print(f"ğŸ“¥ ì´ë¯¸ì§€ ì €ì¥ ìœ„ì¹˜: {output_base}")
    print(f"ğŸ“Š ì²˜ë¦¬ ë²”ìœ„: {start_row}í–‰ ~ {end_row}í–‰\n")
    
    with open(csv_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        rows = list(reader)
    
    total_rows = len(rows)
    if end_row > total_rows:
        end_row = total_rows
        print(f"âš ï¸  ë í–‰ì´ íŒŒì¼ í¬ê¸°ë¥¼ ì´ˆê³¼í•˜ì—¬ {total_rows}í–‰ìœ¼ë¡œ ì¡°ì •í–ˆìŠµë‹ˆë‹¤.\n")
    
    # ì²˜ë¦¬í•  í–‰ ë²”ìœ„ (0-based ì¸ë±ìŠ¤)
    start_idx = start_row - 2  # í—¤ë” ì œì™¸í•˜ê³  0-basedë¡œ ë³€í™˜
    end_idx = end_row - 1  # í—¤ë” ì œì™¸í•˜ê³  0-basedë¡œ ë³€í™˜
    
    success_count = 0
    fail_count = 0
    
    for idx in range(start_idx, end_idx + 1):
        if idx >= len(rows):
            break
        
        row = rows[idx]
        # url ì»¬ëŸ¼ì„ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ shop ì»¬ëŸ¼ í™•ì¸
        shop_url = row.get('url', '').strip() or row.get('shop', '').strip()
        search_keyword = row.get('search_keyword', '').strip()
        name = row.get('name', '').strip()
        img_url = row.get('img', '').strip()
        
        if not shop_url:
            print(f"â­ï¸  [{idx + 2}í–‰] URLì´ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
            continue
        
        if not search_keyword:
            search_keyword = "unknown"
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        keyword_dir = sanitize_directory_name(search_keyword)
        output_dir = output_base / keyword_dir
        output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"\n[{idx + 2}í–‰] {name[:50]}...")
        print(f"  ğŸ” ê²€ìƒ‰ì–´: {search_keyword}")
        print(f"  ğŸŒ URL: {shop_url}")
        
        # 1. img ì»¬ëŸ¼ì— ì´ë¯¸ì§€ URLì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        if img_url and img_url.startswith('http'):
            print(f"  ğŸ“· img ì»¬ëŸ¼ì—ì„œ ì´ë¯¸ì§€ URL ë°œê²¬")
            filename = sanitize_filename(name) if name else f"image_{idx + 2}"
            output_path = output_dir / f"{filename}_{idx + 2}.jpg"
            
            if download_image(img_url, output_path):
                print(f"  âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {output_path.name}")
                success_count += 1
            else:
                print(f"  âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
                fail_count += 1
        else:
            # 2. shop URLì„ ë°©ë¬¸í•˜ì—¬ ì´ë¯¸ì§€ ì°¾ê¸°
            print(f"  ğŸ” í˜ì´ì§€ì—ì„œ ì´ë¯¸ì§€ ì°¾ëŠ” ì¤‘...")
            
            # Musinsaë‚˜ OliveYoungì¸ ê²½ìš° ì „ìš© í•¨ìˆ˜ ì‚¬ìš©
            if PRODUCT_IMAGE_MODULE_AVAILABLE:
                if 'musinsa.com' in shop_url:
                    print(f"  ğŸª Musinsa ì „ìš© API ì‚¬ìš©")
                    image_urls = get_musinsa_product_images(shop_url)
                elif 'oliveyoung.co.kr' in shop_url:
                    print(f"  ğŸª OliveYoung ì „ìš© API ì‚¬ìš©")
                    image_urls = get_oliveyoung_product_images(shop_url)
                else:
                    image_urls = get_image_urls_from_page(shop_url)
            else:
                image_urls = get_image_urls_from_page(shop_url)
            
            if not image_urls:
                print(f"  âš ï¸  ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                fail_count += 1
            else:
                # ìƒí’ˆ ì´ë¯¸ì§€ë¡œ ë³´ì´ëŠ” ê²ƒë“¤ í•„í„°ë§ (í° ì´ë¯¸ì§€ ìš°ì„ )
                # ì¼ë°˜ì ìœ¼ë¡œ ìƒí’ˆ ì´ë¯¸ì§€ëŠ” íŠ¹ì • íŒ¨í„´ì„ ê°€ì§
                product_images = []
                for img_url in image_urls:
                    # ìƒí’ˆ ì´ë¯¸ì§€ë¡œ ë³´ì´ëŠ” URL í•„í„°ë§
                    if any(keyword in img_url.lower() for keyword in ['product', 'goods', 'item', 'detail', 'main', 'cover']):
                        product_images.append(img_url)
                    elif 'jpg' in img_url.lower() or 'jpeg' in img_url.lower() or 'png' in img_url.lower():
                        # í° ì´ë¯¸ì§€ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ (íŒŒì¼ëª…ì— ìˆ«ìê°€ ë§ì€ ê²½ìš°)
                        if len(re.findall(r'\d+', img_url)) > 2:
                            product_images.append(img_url)
                
                # í•„í„°ë§ëœ ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ ëª¨ë“  ì´ë¯¸ì§€ ì‚¬ìš©
                if not product_images:
                    product_images = image_urls[:5]  # ìµœëŒ€ 5ê°œë§Œ
                else:
                    product_images = product_images[:5]  # ìµœëŒ€ 5ê°œë§Œ
                
                print(f"  ğŸ“· {len(product_images)}ê°œì˜ ì´ë¯¸ì§€ ë°œê²¬")
                
                downloaded = False
                for img_idx, img_url in enumerate(product_images):
                    filename = sanitize_filename(name) if name else f"image_{idx + 2}"
                    if len(product_images) > 1:
                        filename = f"{filename}_{img_idx + 1}"
                    output_path = output_dir / f"{filename}_{idx + 2}.jpg"
                    
                    if download_image(img_url, output_path):
                        print(f"  âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ [{img_idx + 1}/{len(product_images)}]: {output_path.name}")
                        downloaded = True
                        success_count += 1
                        break  # ì²« ë²ˆì§¸ ì„±ê³µí•œ ì´ë¯¸ì§€ë§Œ ì €ì¥
                
                if not downloaded:
                    print(f"  âŒ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
                    fail_count += 1
        
        # ìš”ì²­ ê°„ ì§€ì—°
        if idx < end_idx:
            time.sleep(delay)
    
    print("\n" + "=" * 70)
    print("ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
    print("=" * 70)
    print(f"âœ… ì„±ê³µ: {success_count}ê°œ")
    print(f"âŒ ì‹¤íŒ¨: {fail_count}ê°œ")
    print(f"ğŸ“‚ ì €ì¥ ìœ„ì¹˜: {output_base}")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="CSV íŒŒì¼ì˜ shop URLì—ì„œ ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.")
    parser.add_argument(
        "--csv",
        type=str,
        default="tests/scraper_data.csv",
        help="CSV íŒŒì¼ ê²½ë¡œ",
    )
    parser.add_argument(
        "--output",
        type=str,
        default="downloaded_images",
        help="ì´ë¯¸ì§€ë¥¼ ì €ì¥í•  ê¸°ë³¸ ë””ë ‰í† ë¦¬",
    )
    parser.add_argument(
        "--start-row",
        type=int,
        default=2,
        help="ì‹œì‘ í–‰ (1-based, í—¤ë” í¬í•¨)",
    )
    parser.add_argument(
        "--end-row",
        type=int,
        default=67,
        help="ë í–‰ (1-based, í—¤ë” í¬í•¨)",
    )
    parser.add_argument(
        "--delay",
        type=float,
        default=1.0,
        help="ìš”ì²­ ê°„ ì§€ì—° ì‹œê°„ (ì´ˆ)",
    )
    
    args = parser.parse_args()
    
    download_images_from_csv(
        csv_path=args.csv,
        output_base_dir=args.output,
        start_row=args.start_row,
        end_row=args.end_row,
        delay=args.delay,
    )

